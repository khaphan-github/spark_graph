{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "658965a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b976db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c5009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleGraphProcessing\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37fa1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_graph():\n",
    "    \"\"\"Simple graph processing from large_graph.txt\"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ Processing Large Graph\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"source\", IntegerType(), True),\n",
    "        StructField(\"target\", IntegerType(), True),\n",
    "        StructField(\"edge_type\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    print(\"ðŸ“– Reading: /opt/spark/data/large_graph.txt\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = spark.read \\\n",
    "        .option(\"delimiter\", \"|\") \\\n",
    "        .option(\"header\", \"false\") \\\n",
    "        .schema(schema) \\\n",
    "        .csv(\"/opt/spark/data/large_graph.txt\")\n",
    "    \n",
    "    # Cache for faster calculations\n",
    "    df.cache()\n",
    "    \n",
    "    # Get total count\n",
    "    total_edges = df.count()\n",
    "    read_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"âœ… Loaded {total_edges:,} edges in {read_time:.4f}s\")\n",
    "    print(f\"ðŸ“Š Read rate: {total_edges/read_time:,.0f} edges/second\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"\\nðŸ“‹ Sample Data:\")\n",
    "    df.show(5)\n",
    "    \n",
    "    # Calculate for each edge type\n",
    "    print(f\"\\nðŸ”¢ Edge Calculations:\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    edge_counts = df.groupBy(\"edge_type\").count().collect()\n",
    "    calc_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Edge counts (calculated in {calc_time:.6f}s):\")\n",
    "    for row in sorted(edge_counts, key=lambda x: x['count'], reverse=True):\n",
    "        print(f\"   {row['edge_type']}: {row['count']:,} edges\")\n",
    "    \n",
    "    # Show the edge counts DataFrame for verification\n",
    "    print(f\"\\nðŸ“Š Edge Counts DataFrame:\")\n",
    "    edge_counts_df = df.groupBy(\"edge_type\").count().orderBy(\"count\", ascending=False)\n",
    "    edge_counts_df.show()\n",
    "    \n",
    "    # Quick stats\n",
    "    print(f\"\\nðŸ“ˆ Quick Stats:\")\n",
    "    \n",
    "    # Unique nodes\n",
    "    start_time = time.time()\n",
    "    unique_nodes = df.select(\"source\").union(df.select(\"target\")).distinct().count()\n",
    "    stat_time = time.time() - start_time\n",
    "    print(f\"   Unique nodes: {unique_nodes:,} (calculated in {stat_time:.6f}s)\")\n",
    "    \n",
    "    # Show sample of unique nodes\n",
    "    print(f\"\\nðŸ”— Sample Unique Nodes:\")\n",
    "    unique_nodes_df = df.select(\"source\").union(df.select(\"target\")).distinct().orderBy(\"source\")\n",
    "    unique_nodes_df.show(10)\n",
    "    \n",
    "    # Loop through each edge and count\n",
    "    print(f\"\\nðŸ”„ Loop-based Edge Counting:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Collect all rows to loop through them\n",
    "    all_edges = df.collect()\n",
    "    \n",
    "    # Initialize counters\n",
    "    edge_type_counts = {}\n",
    "    total_loop_count = 0\n",
    "    \n",
    "    # Loop through each edge\n",
    "    for edge in all_edges:\n",
    "        total_loop_count += 1\n",
    "        edge_type = edge['edge_type']\n",
    "        \n",
    "        # Count edge types\n",
    "        if edge_type in edge_type_counts:\n",
    "            edge_type_counts[edge_type] += 1\n",
    "        else:\n",
    "            edge_type_counts[edge_type] = 1\n",
    "    \n",
    "    loop_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   Total edges counted in loop: {total_loop_count:,}\")\n",
    "    print(f\"   Loop processing time: {loop_time:.4f}s\")\n",
    "    print(f\"   Loop rate: {total_loop_count/loop_time:,.0f} edges/second\")\n",
    "    print(f\"   Edge type counts from loop:\")\n",
    "    \n",
    "    # Sort and display loop results\n",
    "    for edge_type, count in sorted(edge_type_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"      {edge_type}: {count:,} edges\")\n",
    "    \n",
    "    # Fastest operations\n",
    "    print(f\"\\nâš¡ Fast Operations:\")\n",
    "    \n",
    "    operations = [\n",
    "        (\"Count edges\", lambda: df.count()),\n",
    "        (\"Take first\", lambda: df.take(1)),\n",
    "        (\"Is empty\", lambda: df.rdd.isEmpty())\n",
    "    ]\n",
    "    \n",
    "    fastest_time = float('inf')\n",
    "    fastest_op = \"\"\n",
    "    \n",
    "    for op_name, operation in operations:\n",
    "        start_time = time.time()\n",
    "        result = operation()\n",
    "        op_time = time.time() - start_time\n",
    "        print(f\"   {op_name}: {op_time:.6f}s - Result: {result}\")\n",
    "        \n",
    "        if op_time < fastest_time:\n",
    "            fastest_time = op_time\n",
    "            fastest_op = op_name\n",
    "    \n",
    "    print(f\"\\nðŸ† Fastest: {fastest_op} in {fastest_time:.6f}s\")\n",
    "    \n",
    "    # Final summary output\n",
    "    print(f\"\\nðŸ“‹ Processing Summary:\")\n",
    "    print(f\"   Total edges processed: {total_edges:,}\")\n",
    "    print(f\"   Total edges from loop: {total_loop_count:,}\")\n",
    "    print(f\"   Unique nodes found: {unique_nodes:,}\")\n",
    "    print(f\"   Edge types: {len(edge_counts)}\")\n",
    "    print(f\"   Data read time: {read_time:.4f}s\")\n",
    "    print(f\"   DataFrame calculations time: {calc_time:.4f}s\")\n",
    "    print(f\"   Loop processing time: {loop_time:.4f}s\")\n",
    "    print(f\"   Stats time: {stat_time:.4f}s\")\n",
    "    print(f\"   Total processing time: {time.time() - start_time + read_time + calc_time + stat_time + loop_time:.4f}s\")\n",
    "    \n",
    "    # Verify counts match\n",
    "    print(f\"\\nðŸ” Verification:\")\n",
    "    print(f\"   DataFrame count == Loop count: {total_edges == total_loop_count}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3809709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Processing Large Graph\n",
      "========================================\n",
      "ðŸ“– Reading: /opt/spark/data/large_graph.txt\n",
      "âœ… Loaded 100,001 edges in 0.1873s\n",
      "ðŸ“Š Read rate: 534,029 edges/second\n",
      "\n",
      "ðŸ“‹ Sample Data:\n",
      "+------+------+----------+\n",
      "|source|target| edge_type|\n",
      "+------+------+----------+\n",
      "|  null|  null|edge_label|\n",
      "|     0|    23|friendship|\n",
      "|     1|    40|   follows|\n",
      "|     2|    57|     likes|\n",
      "|     3|    74|    shares|\n",
      "+------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "ðŸ”¢ Edge Calculations:\n",
      "Edge counts (calculated in 0.087180s):\n",
      "   likes: 20,000 edges\n",
      "   follows: 20,000 edges\n",
      "   mentions: 20,000 edges\n",
      "   shares: 20,000 edges\n",
      "   friendship: 20,000 edges\n",
      "   edge_label: 1 edges\n",
      "\n",
      "ðŸ“Š Edge Counts DataFrame:\n",
      "+----------+-----+\n",
      "| edge_type|count|\n",
      "+----------+-----+\n",
      "|     likes|20000|\n",
      "|   follows|20000|\n",
      "|  mentions|20000|\n",
      "|    shares|20000|\n",
      "|friendship|20000|\n",
      "|edge_label|    1|\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "ðŸ“ˆ Quick Stats:\n",
      "   Unique nodes: 10,001 (calculated in 0.100773s)\n",
      "\n",
      "ðŸ”— Sample Unique Nodes:\n",
      "+------+\n",
      "|source|\n",
      "+------+\n",
      "|  null|\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "ðŸ”„ Loop-based Edge Counting:\n",
      "   Total edges counted in loop: 100,001\n",
      "   Loop processing time: 0.2347s\n",
      "   Loop rate: 426,089 edges/second\n",
      "   Edge type counts from loop:\n",
      "      friendship: 20,000 edges\n",
      "      follows: 20,000 edges\n",
      "      likes: 20,000 edges\n",
      "      shares: 20,000 edges\n",
      "      mentions: 20,000 edges\n",
      "      edge_label: 1 edges\n",
      "\n",
      "âš¡ Fast Operations:\n",
      "   Count edges: 0.030813s - Result: 100001\n",
      "   Take first: 0.015074s - Result: [Row(source=None, target=None, edge_type='edge_label')]\n",
      "   Is empty: 0.279370s - Result: False\n",
      "\n",
      "ðŸ† Fastest: Take first in 0.015074s\n",
      "\n",
      "ðŸ“‹ Processing Summary:\n",
      "   Total edges processed: 100,001\n",
      "   Total edges from loop: 100,001\n",
      "   Unique nodes found: 10,001\n",
      "   Edge types: 6\n",
      "   Data read time: 0.1873s\n",
      "   DataFrame calculations time: 0.0872s\n",
      "   Loop processing time: 0.2347s\n",
      "   Stats time: 0.1008s\n",
      "   Total processing time: 0.8894s\n",
      "\n",
      "ðŸ” Verification:\n",
      "   DataFrame count == Loop count: True\n",
      "\n",
      "âœ… Processing completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph_df = process_graph()\n",
    "print(\"\\nâœ… Processing completed!\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
